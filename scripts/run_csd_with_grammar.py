#!/usr/bin/env python3
"""
Run a compiled CSD strategy with a specific grammar.

This script allows you to:
1. Take a compiled CSD strategy (from a successful synthesis run)
2. Specify a grammar file (.lark) for constraint validation
3. Run constrained generation with real grammar enforcement

Usage:
    # With a .lark grammar file
    python scripts/run_csd_with_grammar.py --run-dir outputs/generated-csd/runs/XXXXX --grammar grammars/json.lark
    
    # With a built-in format
    python scripts/run_csd_with_grammar.py --run-dir outputs/generated-csd/runs/XXXXX --format json
    
    # Custom vocabulary from HuggingFace tokenizer
    python scripts/run_csd_with_grammar.py --run-dir outputs/generated-csd/runs/XXXXX --format json --tokenizer Qwen/Qwen2.5-Coder-7B-Instruct
"""

import argparse
import sys
import json
import random
from pathlib import Path
from typing import Optional

# Add project root to path
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))


def create_lark_dafny_parser(grammar_source: str, VerifiedDecoderAgent, _dafny, start: str = "start"):
    """
    Create a Dafny-compatible parser from a Lark grammar.
    
    Args:
        grammar_source: Either a grammar string or path to .lark file
        VerifiedDecoderAgent: The imported Dafny module
        _dafny: The Dafny runtime module
        start: Start rule name
    """
    from lark import Lark
    from lark.exceptions import UnexpectedCharacters, UnexpectedToken, UnexpectedEOF
    
    # Load grammar - check if it's a file path (short string without newlines)
    if '\n' not in grammar_source and len(grammar_source) < 500:
        grammar_path = Path(grammar_source)
        if grammar_path.exists():
            grammar = grammar_path.read_text()
        else:
            grammar = grammar_source
    else:
        grammar = grammar_source
    
    # Create Lark parser
    lark_parser = Lark(grammar, start=start, parser='lalr')
    
    class LarkDafnyParser(VerifiedDecoderAgent.Parser):
        """Parser using Lark grammar, compatible with Dafny-compiled code."""
        
        def __init__(self, lm_tokens):
            super().__init__()
            self._lm_tokens = lm_tokens
            self._token_list = list(lm_tokens)
            self._lark = lark_parser
            self._UnexpectedCharacters = UnexpectedCharacters
            self._UnexpectedToken = UnexpectedToken
            self._UnexpectedEOF = UnexpectedEOF
        
        def _tokens_to_text(self, tokens) -> str:
            """Convert Dafny token sequence to text."""
            try:
                return "".join(str(tokens[i]) for i in range(len(tokens)))
            except (TypeError, AttributeError):
                return str(tokens)
        
        def _is_valid_prefix(self, text: str) -> bool:
            """Check if text is a valid prefix of the grammar."""
            if not text:
                return True
            try:
                self._lark.parse(text)
                return True
            except self._UnexpectedEOF:
                # Hit end of input while expecting more - valid prefix
                return True
            except self._UnexpectedToken as e:
                if e.token.type == '$END':
                    return True
                return False
            except self._UnexpectedCharacters:
                return False
            except Exception:
                return False
        
        def _is_complete(self, text: str) -> bool:
            """Check if text is a complete valid parse."""
            if not text:
                return False
            try:
                self._lark.parse(text)
                return True
            except Exception:
                return False
        
        def IsValidPrefix(self, prefix) -> bool:
            """Dafny interface: Check if prefix is valid."""
            if len(prefix) == 0:
                return True
            text = self._tokens_to_text(prefix)
            return self._is_valid_prefix(text)
        
        def IsCompletePrefix(self, prefix) -> bool:
            """Dafny interface: Check if prefix is complete."""
            if len(prefix) == 0:
                return False
            text = self._tokens_to_text(prefix)
            return self._is_complete(text)
        
        def ValidNextTokens(self, prefix):
            """Dafny interface: Get valid next tokens."""
            current_text = self._tokens_to_text(prefix) if len(prefix) > 0 else ""
            
            if current_text and not self._is_valid_prefix(current_text):
                return _dafny.SeqWithoutIsStrInference([])
            
            valid = []
            for token in self._token_list:
                token_str = str(token)
                if not token_str:
                    continue
                extended = current_text + token_str
                if self._is_valid_prefix(extended):
                    valid.append(token)
            
            return _dafny.SeqWithoutIsStrInference(valid)
    
    return LarkDafnyParser


def get_builtin_grammar(format_name: str) -> str:
    """Get built-in grammar for common formats."""
    grammars = {
        "json": r'''
            start: value
            ?value: object | array | string | number | "true" -> true | "false" -> false | "null" -> null
            object: "{" [pair ("," pair)*] "}"
            pair: string ":" value
            array: "[" [value ("," value)*] "]"
            string: ESCAPED_STRING
            number: SIGNED_NUMBER
            %import common.ESCAPED_STRING
            %import common.SIGNED_NUMBER
            %import common.WS
            %ignore WS
        ''',
        "sql": r'''
            start: select_stmt
            select_stmt: "SELECT"i columns "FROM"i table [where_clause]
            columns: "*" | column ("," column)*
            column: NAME
            table: NAME
            where_clause: "WHERE"i condition
            condition: NAME comp_op value
            comp_op: "=" | "!=" | "<" | ">" | "<=" | ">="
            value: NAME | NUMBER | STRING
            %import common.CNAME -> NAME
            %import common.NUMBER
            %import common.ESCAPED_STRING -> STRING
            %import common.WS
            %ignore WS
        ''',
        "math": r'''
            start: expr
            ?expr: term | expr "+" term | expr "-" term
            ?term: factor | term "*" factor | term "/" factor
            ?factor: NUMBER | "(" expr ")" | "-" factor
            %import common.NUMBER
            %import common.WS
            %ignore WS
        ''',
    }
    
    if format_name.lower() not in grammars:
        raise ValueError(f"Unknown format: {format_name}. Available: {list(grammars.keys())}")
    
    return grammars[format_name.lower()]


def create_vocabulary(vocab_type: str = "default", tokenizer_name: Optional[str] = None, size: int = 500) -> list[str]:
    """Create vocabulary for the LM."""
    if vocab_type == "tokenizer" and tokenizer_name:
        from transformers import AutoTokenizer
        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, trust_remote_code=True)
        vocab = []
        for i in range(min(len(tokenizer), size)):
            try:
                token = tokenizer.decode([i])
                if token:
                    vocab.append(token)
            except:
                pass
        return vocab
    
    # Default vocabulary with common tokens
    vocab = list('{}[]():,."\'+-*/=<>!&|^~%@#$_\\;? \t\n')
    vocab.extend(list('0123456789'))
    vocab.extend(list('abcdefghijklmnopqrstuvwxyz'))
    vocab.extend(list('ABCDEFGHIJKLMNOPQRSTUVWXYZ'))
    vocab.extend(['true', 'false', 'null', 'SELECT', 'FROM', 'WHERE', 'AND', 'OR'])
    vocab.append('<EOS>')
    
    while len(vocab) < size:
        vocab.append(f'<T{len(vocab)}>')
    
    return vocab[:size]


def run_csd_with_grammar(
    run_dir: Path,
    grammar_source: str,
    max_steps: int = 50,
    vocab_size: int = 500,
    tokenizer_name: Optional[str] = None,
    seed: int = 42,
    start_rule: str = "start"
) -> dict:
    """
    Run a compiled CSD strategy with a specific grammar.
    
    Args:
        run_dir: Path to the run directory containing compiled CSD
        grammar_source: Grammar file path or built-in format name
        max_steps: Maximum generation steps
        vocab_size: Vocabulary size
        tokenizer_name: HuggingFace tokenizer for vocabulary
        seed: Random seed
        start_rule: Grammar start rule
        
    Returns:
        Dictionary with results
    """
    random.seed(seed)
    
    run_dir = Path(run_dir)
    module_dir = run_dir / "generated_csd"
    
    if not module_dir.exists():
        return {"success": False, "error": f"Module directory not found: {module_dir}"}
    
    # Add module dir to path
    if str(module_dir) not in sys.path:
        sys.path.insert(0, str(module_dir))
    
    try:
        import _dafny
        import VerifiedDecoderAgent
        import GeneratedCSD
        
        # Determine grammar source
        if Path(grammar_source).exists():
            grammar = Path(grammar_source).read_text()
        else:
            grammar = get_builtin_grammar(grammar_source)
        
        # Create vocabulary
        vocab = create_vocabulary(
            vocab_type="tokenizer" if tokenizer_name else "default",
            tokenizer_name=tokenizer_name,
            size=vocab_size
        )
        
        # Helper for BigRational
        def bigrational_to_float(br):
            s = str(br)
            if '/' in s:
                num, den = s.split('/')
                return float(num) / float(den)
            return float(s)
        
        # Create LM
        class TestLM(VerifiedDecoderAgent.LM):
            def __init__(self, tokens):
                super().__init__()
                self._Tokens = _dafny.SeqWithoutIsStrInference(tokens)
                self._Ids = _dafny.SeqWithoutIsStrInference(list(range(len(tokens))))
                self.Logits = _dafny.Array(None, len(tokens))
                for i in range(len(tokens)):
                    self.Logits[i] = _dafny.BigRational(0)
            
            def GenerateLogits(self, input_prefix):
                for i in range(self.Logits.length(0)):
                    self.Logits[i] = _dafny.BigRational(random.gauss(0, 1))
            
            def ChooseNextToken(self):
                best_idx = 0
                best_logit = -1e10
                masked_val = _dafny.BigRational('-1e9')
                for i in range(self.Logits.length(0)):
                    if self.Logits[i] != masked_val:
                        val = bigrational_to_float(self.Logits[i])
                        if val > best_logit:
                            best_logit = val
                            best_idx = i
                return self._Tokens[best_idx]
        
        # Create parser from grammar
        LarkDafnyParser = create_lark_dafny_parser(grammar, VerifiedDecoderAgent, _dafny, start_rule)
        
        lm = TestLM(vocab)
        parser = LarkDafnyParser(lm._Tokens)
        prompt = _dafny.SeqWithoutIsStrInference([])
        
        # Run the CSD strategy
        output = GeneratedCSD.default__.MyCSDStrategy(lm, parser, prompt, max_steps)
        output_list = [str(t) for t in output]
        output_text = "".join(output_list)
        
        # Validate output
        is_valid = parser._is_valid_prefix(output_text)
        is_complete = parser._is_complete(output_text)
        
        return {
            "success": True,
            "output_tokens": output_list,
            "output_text": output_text,
            "output_length": len(output_list),
            "is_valid_prefix": is_valid,
            "is_complete": is_complete,
            "grammar_source": str(grammar_source),
        }
        
    except Exception as e:
        import traceback
        return {
            "success": False,
            "error": str(e),
            "traceback": traceback.format_exc()
        }


def main():
    parser = argparse.ArgumentParser(
        description="Run a compiled CSD strategy with a specific grammar",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Use a .lark grammar file
  python scripts/run_csd_with_grammar.py --run-dir outputs/generated-csd/runs/20260105_204255_8b7116 --grammar grammars/json.lark
  
  # Use a built-in format
  python scripts/run_csd_with_grammar.py --run-dir outputs/generated-csd/runs/20260105_204255_8b7116 --format json
  
  # With HuggingFace tokenizer vocabulary
  python scripts/run_csd_with_grammar.py --run-dir outputs/generated-csd/runs/20260105_204255_8b7116 --format json --tokenizer Qwen/Qwen2.5-Coder-7B-Instruct
"""
    )
    
    parser.add_argument("--run-dir", "-r", type=Path, required=True,
                        help="Path to the run directory containing compiled CSD")
    
    grammar_group = parser.add_mutually_exclusive_group(required=True)
    grammar_group.add_argument("--grammar", "-g", type=str,
                               help="Path to .lark grammar file")
    grammar_group.add_argument("--format", "-f", type=str, choices=["json", "sql", "math"],
                               help="Built-in format name")
    
    parser.add_argument("--max-steps", type=int, default=50,
                        help="Maximum generation steps (default: 50)")
    parser.add_argument("--vocab-size", type=int, default=500,
                        help="Vocabulary size (default: 500)")
    parser.add_argument("--tokenizer", "-t", type=str, default=None,
                        help="HuggingFace tokenizer for vocabulary")
    parser.add_argument("--seed", type=int, default=42,
                        help="Random seed (default: 42)")
    parser.add_argument("--start-rule", type=str, default="start",
                        help="Grammar start rule (default: start)")
    parser.add_argument("--json", action="store_true",
                        help="Output as JSON")
    
    args = parser.parse_args()
    
    # Determine grammar source
    grammar_source = args.grammar if args.grammar else args.format
    
    print(f"Running CSD with grammar: {grammar_source}")
    print(f"Run directory: {args.run_dir}")
    print()
    
    results = run_csd_with_grammar(
        run_dir=args.run_dir,
        grammar_source=grammar_source,
        max_steps=args.max_steps,
        vocab_size=args.vocab_size,
        tokenizer_name=args.tokenizer,
        seed=args.seed,
        start_rule=args.start_rule
    )
    
    if args.json:
        print(json.dumps(results, indent=2, default=str))
    else:
        if results["success"]:
            print(f"✓ Generation successful")
            print(f"  Output length: {results['output_length']} tokens")
            print(f"  Output: {repr(results['output_text'][:80])}...")
            print(f"  Valid prefix: {results['is_valid_prefix']}")
            print(f"  Complete: {results['is_complete']}")
        else:
            print(f"✗ Generation failed: {results.get('error', 'Unknown error')}")
            if results.get("traceback"):
                print(results["traceback"])
    
    sys.exit(0 if results["success"] and results.get("is_valid_prefix") else 1)


if __name__ == "__main__":
    main()

